{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6M-agd6cQy2"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D,Flatten,Dense,MaxPool2D,BatchNormalization, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.applications.nasnet import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.applications.nasnet import NASNetMobile\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import tempfile\n",
        "import sys\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "from tensorflow.lite.python import schema_py_generated\n",
        "if sys.version_info.major >= 3:\n",
        "    import pathlib\n",
        "else:\n",
        "    import pathlib2 as pathlib\n",
        "# schema_py_generated.Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ixqg9Ln8GU1",
        "outputId": "240bdf7a-d463-4e45-eea7-a0477f044887"
      },
      "source": [
        "!pip install tensorflow_model_optimization\n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.7.4-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy~=1.23\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.16.0)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (0.1.8)\n",
            "Installing collected packages: numpy, tensorflow_model_optimization\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3 tensorflow_model_optimization-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CwVTYfyVwyrZ",
        "outputId": "f95f160d-5c2d-493b-a012-27b240aa346a"
      },
      "source": [
        "!pip install keras-flops\n",
        "from keras_flops import get_flops"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-flops\n",
            "  Downloading keras_flops-0.1.2-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: tensorflow<3.0,>=2.2 in /usr/local/lib/python3.10/dist-packages (from keras-flops) (2.12.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (23.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (67.7.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.12.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (23.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.32.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.16.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.3.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.4.8)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.54.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (16.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.6.3)\n",
            "Collecting numpy<1.24,>=1.22\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.12.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.20.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3.0,>=2.2->keras-flops) (4.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<3.0,>=2.2->keras-flops) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<3.0,>=2.2->keras-flops) (1.10.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (0.7.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (2.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (2.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (1.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (0.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3.0,>=2.2->keras-flops) (3.2.2)\n",
            "Installing collected packages: numpy, keras-flops\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "Successfully installed keras-flops-0.1.2 numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HfzFJhznAXZ"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "#from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "saQLarxOqJT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "dLNLxeLGqJtr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddD9VrDtnAXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "cebdbfe9-c07e-40bd-b743-391ab8a39b37"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-890ebd627f55>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_output\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_SetOutputMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m from .utils._tags import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadpool_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m from .validation import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreadpoolctl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    483\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 485\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuppress_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_private\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_assert_valid_refcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gen_alignment_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextbuild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorators\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m from numpy.core import(\n\u001b[1;32m     22\u001b[0m      intp, float32, empty, arange, array_repr, ndarray, isnat, array)\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlapack_lite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfhxP-o5nAXa"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RpfFRIpnAXb"
      },
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhCh8c59nAXc",
        "outputId": "c5e7ce40-79cb-4e23-90df-cb3717769290"
      },
      "source": [
        "print(tf. __version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvdY42PdnAXe"
      },
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "\n",
        "config = ConfigProto()\n",
        "# config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "# session.close()\n",
        "\n",
        "# 为确保pytorch运行 必须限制tf占用的内存\n",
        "# config = ConfigProto()\n",
        "# config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
        "# # session = InteractiveSession().close()\n",
        "# session = InteractiveSession(config=config)\n",
        "# session.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IGd4To4nAXf"
      },
      "source": [
        "def get_file_size(file_path):\n",
        "    size = os.path.getsize(file_path)\n",
        "    return size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UvIXCxtnAXf"
      },
      "source": [
        "def convert_bytes(size, unit=None):\n",
        "    if unit == 'KB':\n",
        "        return print('File size:' + str( round(size/ 1024, 3))+ 'kb')\n",
        "    elif unit == 'MB':\n",
        "        return print('File size:' + str( round(size/ (1024*1024), 3))+ 'Mb')\n",
        "    else:\n",
        "        return print('File size:' + str(size)+ 'bytes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXCb8qFWnAXg"
      },
      "source": [
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "  size = os.path.getsize(zipped_file)\n",
        "  print('File size:' + str( round(size/ (1024*1024), 3))+ 'Mb')\n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdGaMO4FnAXg"
      },
      "source": [
        "def generate_train_data_from_directory(train_data_dir, image_target_size = 224, batch_size = 32, channels = 3, class_mode = 'categorical' ): \n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "            train_data_dir ,\n",
        "            target_size = (image_target_size, image_target_size),\n",
        "            batch_size  = batch_size,\n",
        "            class_mode  = class_mode)\n",
        "\n",
        "    total_images = train_generator.n  \n",
        "    steps = total_images//batch_size \n",
        "#iterations to cover all data, so if batch is 5, it will take total_images/5  iteration \n",
        "\n",
        "    train_images , train_labels = [] , []\n",
        "    for i in range(steps):\n",
        "        a , b = train_generator.next()\n",
        "        train_images.extend(a) \n",
        "        train_labels.extend(b)\n",
        "    \n",
        "    return np.array(train_images), np.array(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDLaStcpnAXh"
      },
      "source": [
        "def generate_test_data_from_directory(test_data_dir, image_target_size = 224, batch_size = 1, channels = 3, class_mode = 'categorical' ): \n",
        "\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "            test_data_dir ,\n",
        "            target_size = (image_target_size, image_target_size),\n",
        "            batch_size  = batch_size,\n",
        "            class_mode  = class_mode)\n",
        "\n",
        "    total_images = test_generator.n  \n",
        "    steps = total_images//batch_size \n",
        "#iterations to cover all data, so if batch is 5, it will take total_images/5  iteration \n",
        "\n",
        "    test_images , test_labels = [] , []\n",
        "    for i in range(steps):\n",
        "        a , b = test_generator.next()\n",
        "        test_images.extend(a) \n",
        "        test_labels.extend(b)\n",
        "    \n",
        "    return np.array(test_images), np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP65BsBur3YJ"
      },
      "source": [
        "def inference_integer_tflite(mode_path, num_test):\n",
        "  interpreter = tf.lite.Interpreter(model_path=mode_path)\n",
        "\n",
        "  interpreter.allocate_tensors()\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  print('input_details:  ', interpreter.get_input_details())\n",
        "  print('output_details: ', interpreter.get_output_details())\n",
        "\n",
        "  total_seen = 0\n",
        "  num_correct = 0\n",
        "  inference_time = []\n",
        "\n",
        "  for batch in test.take(int(num_test)):\n",
        "    image = batch[0].numpy()\n",
        "\n",
        "    start_ms = time.time()\n",
        "    image = np.expand_dims(image,0).astype(np.uint8)\n",
        "    interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], image)\n",
        "    interpreter.invoke()\n",
        "    predictions = interpreter.get_tensor(interpreter.get_output_details()[0][\"index\"])\n",
        "\n",
        "    elapsed_ms = time.time() - start_ms\n",
        "    inference_time.append(elapsed_ms * 1000.0)\n",
        "\n",
        "    if np.argmax(batch[1].numpy()) == np.argmax(predictions):\n",
        "      num_correct += 1\n",
        "    total_seen += 1\n",
        "\n",
        "    if total_seen % 500 == 0:\n",
        "        print(\"Accuracy after %i images: %f\" %\n",
        "              (total_seen, float(num_correct) / float(total_seen)))\n",
        "        \n",
        "\n",
        "  print('Num images: {0:}, Accuracy: {1:.4f}, Latency: {2:.2f} ms'.format(num_test,\n",
        "                                                                         float(num_correct / total_seen),\n",
        "                                                                         np.array(inference_time).mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkZQuRWpnAXh"
      },
      "source": [
        "def inference_tflite(mode_path, num_test):\n",
        "  interpreter = tf.lite.Interpreter(model_path=mode_path)\n",
        "\n",
        "  interpreter.allocate_tensors()\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  print('input_details:  ', interpreter.get_input_details())\n",
        "  print('output_details: ', interpreter.get_output_details())\n",
        "\n",
        "  total_seen = 0\n",
        "  num_correct = 0\n",
        "  inference_time = []\n",
        "\n",
        "  for batch in test.take(int(num_test)):\n",
        "    image = batch[0].numpy()\n",
        "\n",
        "    start_ms = time.time()\n",
        "    image = np.expand_dims(image,0).astype(np.float32)\n",
        "    interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], image)\n",
        "    interpreter.invoke()\n",
        "    predictions = interpreter.get_tensor(interpreter.get_output_details()[0][\"index\"])\n",
        "\n",
        "    elapsed_ms = time.time() - start_ms\n",
        "    inference_time.append(elapsed_ms * 1000.0)\n",
        "\n",
        "    if np.argmax(batch[1].numpy()) == np.argmax(predictions):\n",
        "      num_correct += 1\n",
        "    total_seen += 1\n",
        "\n",
        "    if total_seen % 500 == 0:\n",
        "        print(\"Accuracy after %i images: %f\" %\n",
        "              (total_seen, float(num_correct) / float(total_seen)))\n",
        "        \n",
        "\n",
        "  print('Num images: {0:}, Accuracy: {1:.4f}, Latency: {2:.2f} ms'.format(num_test,\n",
        "                                                                         float(num_correct / total_seen),\n",
        "                                                                         np.array(inference_time).mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwcSdZq2eufB",
        "outputId": "e246685d-6859-4806-aa9e-a028af8cc665"
      },
      "source": [
        "\n",
        "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no2W0DjlbPgS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "f78a0cc4-b876-45b7-c5d2-ae61b2e8d893"
      },
      "source": [
        "\n",
        "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2ccc9b54a3e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1wlS-gxh2jh",
        "outputId": "ae3594d7-dcbe-40b3-878c-388b51ab15f1"
      },
      "source": [
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((35000, 32, 32, 3), (35000, 1))\n",
            "((15000, 32, 32, 3), (15000, 1))\n",
            "((10000, 32, 32, 3), (10000, 1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxMvA_HPnAXj",
        "outputId": "5de5fc0d-1a4c-4bfe-83ed-0be60cfb311b"
      },
      "source": [
        "num_test=len(x_test[:1000])\n",
        "print(num_test)\n",
        "\n",
        "y_train=to_categorical(y_train)\n",
        "y_val=to_categorical(y_val)\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykiZ3yJ7nAXj",
        "outputId": "9a9fd276-ffb5-42c6-fb5b-947f67057fde"
      },
      "source": [
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((35000, 32, 32, 3), (35000, 10))\n",
            "((15000, 32, 32, 3), (15000, 10))\n",
            "((10000, 32, 32, 3), (10000, 10))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc2N1fZWnAXk"
      },
      "source": [
        "train_generator = ImageDataGenerator(\n",
        "                                    rotation_range=2, \n",
        "                                    horizontal_flip=True,\n",
        "                                    zoom_range=.1 )\n",
        "\n",
        "val_generator = ImageDataGenerator(\n",
        "                                    rotation_range=2, \n",
        "                                    horizontal_flip=True,\n",
        "                                    zoom_range=.1)\n",
        "\n",
        "test_generator = ImageDataGenerator(\n",
        "                                    rotation_range=2, \n",
        "                                    horizontal_flip= True,\n",
        "                                    zoom_range=.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRR1KddOoYZd"
      },
      "source": [
        "train_generator.fit(x_train)\n",
        "val_generator.fit(x_val)\n",
        "test_generator.fit(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kjb-ckqq_TQ"
      },
      "source": [
        "batch_size= 100\n",
        "epochs=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMObnLXvnAXk"
      },
      "source": [
        "test = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5QEAhbZexWK",
        "scrolled": false
      },
      "source": [
        "base_model = NASNetMobile(include_top=False, weights=None, input_shape=(32,32,3),classes=y_train.shape[1])\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1000, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "sgd = tf.keras.optimizers.SGD(learning_rate = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)    \n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics = ['accuracy'])    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHpMYJk8qXZ-",
        "outputId": "5c074aca-bf97-4ca0-bf5b-9635f5c758b2"
      },
      "source": [
        "model.fit(train_generator.flow(x_train,y_train,batch_size=batch_size),\n",
        "                      epochs=epochs,\n",
        "                      validation_data=val_generator.flow(x_val,y_val,batch_size=batch_size),verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "350/350 [==============================] - 37s 71ms/step - loss: 1475.1205 - accuracy: 0.0988 - val_loss: 2.3026 - val_accuracy: 0.0979\n",
            "Epoch 2/25\n",
            "350/350 [==============================] - 22s 62ms/step - loss: 2.3039 - accuracy: 0.1012 - val_loss: 2.3026 - val_accuracy: 0.0979\n",
            "Epoch 3/25\n",
            "350/350 [==============================] - 21s 61ms/step - loss: 2.3031 - accuracy: 0.1002 - val_loss: 2.3027 - val_accuracy: 0.0979\n",
            "Epoch 4/25\n",
            "350/350 [==============================] - 21s 59ms/step - loss: 2.3028 - accuracy: 0.0989 - val_loss: 2.3027 - val_accuracy: 0.0979\n",
            "Epoch 5/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3029 - accuracy: 0.0993 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 6/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3027 - accuracy: 0.0979 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 7/25\n",
            "350/350 [==============================] - 20s 57ms/step - loss: 2.3028 - accuracy: 0.1007 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 8/25\n",
            "350/350 [==============================] - 20s 57ms/step - loss: 2.3026 - accuracy: 0.1007 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 9/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 10/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.1005 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 11/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.0993 - val_loss: 2.3028 - val_accuracy: 0.0971\n",
            "Epoch 12/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3028 - accuracy: 0.1004 - val_loss: 2.3028 - val_accuracy: 0.0971\n",
            "Epoch 13/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.1006 - val_loss: 2.3028 - val_accuracy: 0.0971\n",
            "Epoch 14/25\n",
            "350/350 [==============================] - 20s 57ms/step - loss: 2.3027 - accuracy: 0.1006 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 15/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.1002 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 16/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.0995 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 17/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3027 - accuracy: 0.1013 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 18/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.1007 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 19/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.1010 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 20/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.1011 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 21/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.1005 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 22/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.1008 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 23/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.1013 - val_loss: 2.3028 - val_accuracy: 0.0971\n",
            "Epoch 24/25\n",
            "350/350 [==============================] - 20s 57ms/step - loss: 2.3026 - accuracy: 0.1004 - val_loss: 2.3027 - val_accuracy: 0.0971\n",
            "Epoch 25/25\n",
            "350/350 [==============================] - 20s 58ms/step - loss: 2.3026 - accuracy: 0.0999 - val_loss: 2.3027 - val_accuracy: 0.0971\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcd085e1410>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdH-fRnAnAXl"
      },
      "source": [
        "plt.plot(history.history['accuracy'])    #plot oringal model accuracy and loss\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('NASNetMobile no optimized model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Training', 'Test'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vku3fKS8nAXl",
        "scrolled": false
      },
      "source": [
        "plt.plot(history.history['loss'])           #plot oringal model accuracy and loss\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('NASNetMobile no optimized model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Training', 'Test'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_7jN0cU2sF"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_generator.flow(x_test,y_test,batch_size=1), verbose=2)    #oringal model evaluation\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdlX7sRFwQUf"
      },
      "source": [
        "times = {}\n",
        "avg_times = []\n",
        "num_instances = [1,10,100,1000]#,10000]\n",
        "for instance in num_instances:\n",
        "  times[instance] = []\n",
        "num_rep = 100\n",
        "for inst_count in num_instances:\n",
        "  x_test_sample=x_test[0:inst_count]\n",
        "  batch_size = inst_count\n",
        "  for rep in range(num_rep):\n",
        "    start_time = time.time()\n",
        "    test_pred = model.predict(test_generator.flow(x_test_sample, batch_size=batch_size), verbose=0)    #oringal model evaluation\n",
        "    times[inst_count].append(time.time()-start_time)\n",
        "  avg_times.append(sum(times[inst_count])/num_rep)\n",
        "  print(inst_count)\n",
        "print(num_instances)\n",
        "print(avg_times)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_kSxWkUxB4I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "fe03f4f1-52eb-419c-a4e6-b758fed3eee5"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "plt.rcParams['figure.figsize'] = [10,7]\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "\n",
        "  if normalize:\n",
        "      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "      print(\"Normalized confusion matrix\")\n",
        "  else:\n",
        "      print('Confusion matrix, without normalization')\n",
        "\n",
        "  print(cm)\n",
        "\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes, rotation=45)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "\n",
        "  fmt = '.2f' if normalize else 'd'\n",
        "  thresh = cm.max() / 2.\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "      plt.text(j, i, format(cm[i, j], fmt),\n",
        "               horizontalalignment=\"center\",\n",
        "               color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "p_test = model.predict(x_test)\n",
        "# cm = confusion_matrix(y_test, p_test)\n",
        "cm = confusion_matrix(tf.math.argmax(y_test, axis=1), tf.math.argmax(p_test, axis=1))\n",
        "plot_confusion_matrix(cm, list(range(10)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[  0   0   2 908   0   1   0  89   0   0]\n",
            " [  0   0   6 706   0   2   0 286   0   0]\n",
            " [  0   0  19 763  14   1   0 203   0   0]\n",
            " [  0   0   1 909   2   0   0  88   0   0]\n",
            " [  0   0  24 759  14   0   0 203   0   0]\n",
            " [  0   0   2 911   1   0   0  86   0   0]\n",
            " [  0   0  31 782  10   0   0 177   0   0]\n",
            " [  0   0   6 740   0   0   0 254   0   0]\n",
            " [  0   0   0 936   0   1   0  63   0   0]\n",
            " [  0   0   3 793   0   2   0 202   0   0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFdY_yznxB7i"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(tf.math.argmax(p_test, axis=1), tf.math.argmax(y_test, axis=1), average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMbqMSjqxCAD"
      },
      "source": [
        "f1_score(tf.math.argmax(p_test, axis=1), tf.math.argmax(y_test, axis=1), average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR4Gbj3-xP3H"
      },
      "source": [
        "f1_score(tf.math.argmax(p_test, axis=1), tf.math.argmax(y_test, axis=1), average='weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmPvNt1HxP8H"
      },
      "source": [
        "# Calculae FLOPS\n",
        "flops = get_flops(model, batch_size=1)\n",
        "print(f\"FLOPS: {flops / 10 ** 9:.03} G\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jleDPV0-xW7x"
      },
      "source": [
        "# def get_flops_from_model(model_h5_path):\n",
        "#     session = tf.compat.v1.Session()\n",
        "#     graph = tf.compat.v1.get_default_graph()\n",
        "        \n",
        "\n",
        "#     with graph.as_default():\n",
        "#         with session.as_default():\n",
        "#             model = tf.keras.models.load_model(model_h5_path)\n",
        "\n",
        "#             run_meta = tf.compat.v1.RunMetadata()\n",
        "#             opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "        \n",
        "#             # We use the Keras session graph in the call to the profiler.\n",
        "#             flops = tf.compat.v1.profiler.profile(graph=graph,\n",
        "#                           run_meta=run_meta, cmd='op', options=opts)\n",
        "        \n",
        "#             return flops.total_float_ops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfGQm_OIxoIt"
      },
      "source": [
        "models_dir = pathlib.Path(os.path.join('.', 'models'))\n",
        "models_dir.mkdir(exist_ok=True, parents=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UCE-FpSxoMD"
      },
      "source": [
        "model.save(os.path.join(models_dir, 'Mobilenet.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuEbxP9mxrut"
      },
      "source": [
        "# print(f\"FLOPS: {get_flops_from_model(os.path.join(models_dir, 'Mobilenet.h5')) / 10 ** 9:.03} G\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAKSKu2ynAXm"
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_TOsvK9nAXm"
      },
      "source": [
        "quantization aware training(with no quanti)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkVEbX6tnAXn"
      },
      "source": [
        "Quantize some layers\n",
        "Quantizing a model can have a negative effect on accuracy. You can selectively quantize layers of a model to explore the trade-off between accuracy, speed, and model size.\n",
        "\n",
        "Your use case:\n",
        "\n",
        "To deploy to a backend that only works well with fully quantized models (e.g. EdgeTPU v1, most DSPs), try \"Quantize whole model\".\n",
        "Tips for better model accuracy:\n",
        "\n",
        "It's generally better to finetune with quantization aware training as opposed to training from scratch.\n",
        "Try quantizing the later layers instead of the first layers.\n",
        "Avoid quantizing critical layers (e.g. attention mechanism).\n",
        "In the example below, quantize only the Dense layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXYsn8WDnAXn"
      },
      "source": [
        "# def apply_quantization_to_dense(layer):\n",
        "#   if isinstance(layer, tf.keras.layers.Dense):\n",
        "#     return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
        "#   return layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtrJM1fMnAXo"
      },
      "source": [
        "# Use `tf.keras.models.clone_model` to apply `apply_quantization_to_dense` \n",
        "# to the layers of the model.\n",
        "# annotated_model = tf.keras.models.clone_model(\n",
        "#     model,\n",
        "#     clone_function=apply_quantization_to_dense,\n",
        "# )\n",
        "\n",
        "# # Now that the Dense layers are annotated,\n",
        "# # `quantize_apply` actually makes the model quantization aware.\n",
        "# q_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
        "\n",
        "# # `quantize_model` requires a recompile.\n",
        "# q_aware_model.compile('sgd',\n",
        "#               loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "#               metrics=['accuracy'])\n",
        "# q_aware_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_lZN_eKnAXp",
        "scrolled": true
      },
      "source": [
        "# history =q_aware_model.fit(train_generator.flow(x_train,y_train,batch_size=batch_size), \n",
        "#                            validation_data=val_generator.flow(x_val,y_val,batch_size=batch_size), initial_epoch=50, epochs=55)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juvOisfJnAXq"
      },
      "source": [
        "# plt.plot(history.history['accuracy'])\n",
        "# plt.plot(history.history['val_accuracy'])\n",
        "# plt.title('QAT model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['Training', 'Test'], loc='lower right')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgGZNhNmnAXq"
      },
      "source": [
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('QAT model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['Training', 'Test'], loc='lower right')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ1R0SWtnAXr"
      },
      "source": [
        "Convert model to TFlite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9uqnqtunAXr"
      },
      "source": [
        "models_dir = pathlib.Path(os.path.join('.', 'models'))\n",
        "models_dir.mkdir(exist_ok=True, parents=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5gBwZYynAXr"
      },
      "source": [
        "TFlite model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh2wug_KnAXr"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "converter.experimental_new_converter = True\n",
        "tflite_model = converter.convert()\n",
        "with open(os.path.join(models_dir, 'NASNetMobile.tflite'), 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUFyeAj-nAXs"
      },
      "source": [
        "Dynamic range quantization (Post training quantization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKFNKFpmnAXs"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# converter.experimental_new_converter = True\n",
        "# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_dynamic_quant_model = converter.convert()\n",
        "with open(os.path.join(models_dir, 'NASNetMobile_dynamic_quant.tflite'), 'wb') as f:\n",
        "    f.write(tflite_dynamic_quant_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_hpIz7AnAXs"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_dynamic_quant_model)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxB7HmWenAXs"
      },
      "source": [
        "Float16 quantization model (Post training quantization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYTIgmEXnAXs"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# converter.experimental_new_converter = True\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_fp16_quant_model = converter.convert()\n",
        "with open(os.path.join(models_dir, 'NASNetMobile_fp16_quant.tflite'), 'wb') as f:\n",
        "    f.write(tflite_fp16_quant_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfRdYr5vnAXs"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_fp16_quant_model)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYlWFrrKnAXt"
      },
      "source": [
        "Convert using dynamic range quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MG0q1V5nAXt"
      },
      "source": [
        "Convert using float fallback quantization\n",
        "To quantize the variable data (such as model input/output and intermediates between layers), you need to provide a RepresentativeDataset. This is a generator function that provides a set of input data that's large enough to represent typical values. It allows the converter to estimate a dynamic range for all the variable data. (The dataset does not need to be unique compared to the training or evaluation dataset.) To support multiple inputs, each representative data point is a list and elements in the list are fed to the model according to their indices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss8BxBWqnAXt"
      },
      "source": [
        "integer only quantization model (Post training quantization) int 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivcnzJVXnAXt",
        "scrolled": false
      },
      "source": [
        "def representative_dataset():\n",
        "  for data in tf.data.Dataset.from_tensor_slices((x_test)).batch(1).take(100):\n",
        "    yield [tf.dtypes.cast(data, tf.float32)]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "converter.inference_input_type = tf.uint8  # or tf.uint8\n",
        "converter.inference_output_type = tf.uint8  # or tf.uint8\n",
        "tflite_full_integer_quant_model = converter.convert()\n",
        "\n",
        "with open(os.path.join(models_dir, 'NASNetMobile_integer_quant.tflite'), 'wb') as f:\n",
        "    f.write(tflite_full_integer_quant_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej0DmNx3nAXt"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_full_integer_quant_model)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TocicRN2nAXt"
      },
      "source": [
        "Integer quantization model (Quantization aware training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIfifcPenAXt"
      },
      "source": [
        "# #with tfmo.quantization.keras.quantize_scope():\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "\n",
        "# converter.experimental_new_converter = True\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# tflite_q_aware_integer_quant_model = converter.convert()\n",
        "# with open(os.path.join(models_dir, 'NASNetMobile_q_aware_integer_quant.tflite'), 'wb') as f:\n",
        "#     f.write(tflite_q_aware_integer_quant_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ydbYsD4nAXu"
      },
      "source": [
        "test tflite model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPLiiLfynAXv"
      },
      "source": [
        "model_path = os.path.join(models_dir, 'NASNetMobile.tflite')\n",
        "inference_tflite(model_path, int(num_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0SeJfNFnAXv"
      },
      "source": [
        "convert_bytes(get_file_size(os.path.join(models_dir, 'NASNetMobile.tflite')),'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc0z-5PGnAXv"
      },
      "source": [
        "integer only quantization model (Post training quantization) int 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b64ArR4nAXv"
      },
      "source": [
        "model_path = os.path.join(models_dir, 'NASNetMobile_integer_quant.tflite')\n",
        "inference_integer_tflite(model_path, int(num_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXqyNSoInAXw"
      },
      "source": [
        "convert_bytes(get_file_size(os.path.join(models_dir, 'NASNetMobile_integer_quant.tflite')),'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ5hyb50nAXw"
      },
      "source": [
        "dynamic range quantization (Post training quantization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7NCOlSHnAXw"
      },
      "source": [
        "model_path = os.path.join(models_dir, 'NASNetMobile_dynamic_quant.tflite')\n",
        "inference_tflite(model_path, int(num_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZc4AqDynAXw"
      },
      "source": [
        "convert_bytes(get_file_size(os.path.join(models_dir, 'NASNetMobile_dynamic_quant.tflite')),'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hf3u4fVnAXw"
      },
      "source": [
        "Float16 quantization model (Post training quantization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKinoiK5nAXw"
      },
      "source": [
        "model_path = os.path.join(models_dir, 'NASNetMobile_fp16_quant.tflite')\n",
        "inference_tflite(model_path, int(num_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9JgrLN-nAXx"
      },
      "source": [
        "convert_bytes(get_file_size(os.path.join(models_dir, 'NASNetMobile_fp16_quant.tflite')),'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRvAxg3OnAXx"
      },
      "source": [
        "Integer only: 16-bit activations with 8-bit weights (experimental)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZXdszpInAXx"
      },
      "source": [
        "model_path = os.path.join(models_dir, 'NASNetMobile_integer_quant.tflite')\n",
        "inference_integer_tflite(model_path, int(num_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNGcONBCnAXx"
      },
      "source": [
        "convert_bytes(get_file_size(os.path.join(models_dir, 'NASNetMobile_integer_quant.tflite')),'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Cwr9iFonAXx"
      },
      "source": [
        "Integer quantization model (Quantization aware training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPJdylcLnAXx"
      },
      "source": [
        "# model_path = os.path.join(models_dir, 'NASNetMobile_q_aware_integer_quant.tflite')\n",
        "# inference_tflite(model_path, int(num_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpEovvIVnAXy"
      },
      "source": [
        "# convert_bytes(get_file_size(os.path.join(models_dir, 'NASNetMobile_q_aware_integer_quant.tflite')),'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-83LZAgknAXy"
      },
      "source": [
        "pruning and quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyQyjhGMnAXy"
      },
      "source": [
        "# x,y = train_generator.next()\n",
        "image = x_train[0]\n",
        "label = y_train[0]\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "epochs = 5\n",
        "validation_split = 0.25\n",
        "num_images = image.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.10,\n",
        "                                                               final_sparsity=0.300,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile('sgd',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model_for_pruning.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf317_hwgDqQ"
      },
      "source": [
        "print(num_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHEk1yQenAXy"
      },
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "history = model_for_pruning.fit(train_generator.flow(x_train,y_train,batch_size=batch_size), \n",
        "                                validation_data=val_generator.flow(x_val,y_val,batch_size=batch_size),\n",
        "                                callbacks=callbacks,\n",
        "                                epochs=5, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8wqbUpXnAXy"
      },
      "source": [
        "plt.plot(history.history['accuracy'])    #plot oringal model accuracy and loss\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('pruned model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Training', 'Test'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx6eYGQ6nAXy"
      },
      "source": [
        "plt.plot(history.history['loss'])           #plot oringal model accuracy and loss\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('pruned model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Training', 'Test'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGNboWKdnAXz"
      },
      "source": [
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "   test_generator.flow(x_test,y_test,batch_size=1), verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHZDNveBnAXz"
      },
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "\n",
        "model_for_pruning.save(os.path.join(models_dir, 'NASNetMobile_prund.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPzRJHInnAXz",
        "scrolled": true
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "# _, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "with open(os.path.join(models_dir, 'NASNetMobile_pruned.tflite'), 'wb') as f:\n",
        "    f.write(pruned_tflite_model)\n",
        "\n",
        "# print('Saved pruned TFLite model to:', pruned_tflite_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7JkmNZGnAXz"
      },
      "source": [
        "model_path = os.path.join(models_dir, 'NASNetMobile_pruned.tflite')\n",
        "inference_tflite(model_path, int(num_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqSodZvdnAXz"
      },
      "source": [
        "convert_bytes(get_file_size(os.path.join(models_dir, 'NASNetMobile_pruned.tflite')),'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB5_vjR1nAX0"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "# _, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(os.path.join(models_dir, 'NASNetMobile_quant_and_pruned.tflite'), 'wb') as f:\n",
        "    f.write(quantized_and_pruned_tflite_model)\n",
        "\n",
        "\n",
        "# print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
        "\n",
        "# print(\"Size of gzipped baseline Keras model: %.2f bytes\" %(get_gzipped_model_size(keras_file)))\n",
        "# print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JkDpu8jnAX0"
      },
      "source": [
        "model_path = os.path.join(models_dir, 'NASNetMobile_quant_and_pruned.tflite')\n",
        "inference_tflite(model_path, int(num_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4z1PK5TnAX0"
      },
      "source": [
        "convert_bytes(get_file_size(os.path.join(models_dir, 'NASNetMobile_quant_and_pruned.tflite')),'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC7L4UfanAX0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv8fwNOEnAXu"
      },
      "source": [
        "# Save keras model\n",
        "model.save(os.path.join(models_dir, 'NASNetMobile.h5'))\n",
        "# q_aware_model.save(os.path.join(models_dir, 'NASNetMobile_quant.h5'))\n",
        "model_for_pruning.save(os.path.join(models_dir, 'NASNetMobile_pruned.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vG6S9Dg0oDN"
      },
      "source": [
        "convert_bytes(get_file_size(os.path.join(models_dir, 'NASNetMobile_pruned.h5')),'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBaO49JqnAXu"
      },
      "source": [
        "# convert_bytes(get_file_size(os.path.join(models_dir, 'NASNetMobile_quant.h5')),'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyDrtbrrnAXu"
      },
      "source": [
        "convert_bytes(get_file_size(os.path.join(models_dir, 'NASNetMobile.h5')),'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjbZhK-a14UR"
      },
      "source": [
        "get_gzipped_model_size(os.path.join(models_dir, 'NASNetMobile.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuhCY01p15TZ"
      },
      "source": [
        "# get_gzipped_model_size(os.path.join(models_dir, 'NASNetMobile_quant.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ea2mPLs1R6y"
      },
      "source": [
        "get_gzipped_model_size(os.path.join(models_dir, 'NASNetMobile_pruned.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}