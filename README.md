# Imageclassification_Resnet_Moblinet-_NasNet
Resnet, mobilnet, and Nasnet are all included in this project, which have been optimized for Edge devices. There are two major ways to reach the goal: prunning and post train quantization.

Pruning a deep learning model involves removing some of its parameters or connections to reduce the model's size and complexity without sacrificing its performance. There are several reasons why pruning a deep learning model can be beneficial:

  1.Reducing model size and complexity
  2.Improving generalization and reducing overfitting
  3.Reducing inference time and energy consumption
  4.Facilitating model compression and transfer learning
  
  
Post-training quantization is a technique used to reduce the memory and computational requirements of a trained deep learning model by quantizing its weights and activations to use lower precision data types. This is typically done after a model has been trained with floating-point precision, by converting its weights and biases to integers or lower precision floating-point numbers.
  
  1.Reduced model size
  2.Faster inference
  3.Improved energy efficiency
  4.Compatibility with hardware accelerators

